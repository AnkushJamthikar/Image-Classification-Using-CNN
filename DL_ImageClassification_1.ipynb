{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_ImageClassification_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+Omz11dPrD7oE/pD0wPBw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9d054e7a7f943ad8aeac70c659cad27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_89463feb55b341bd9ef5b54f7622db33",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_33bfc4d2244a46a08f03ab284ef40272",
              "IPY_MODEL_2f04fbadf5ea426b84019d75fcfdac1c",
              "IPY_MODEL_a0d18009fa1f4778920b950fd7088082"
            ]
          }
        },
        "89463feb55b341bd9ef5b54f7622db33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33bfc4d2244a46a08f03ab284ef40272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_72e075fc048b4c53a4796d2fc46bb46d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25636c4c5e5840af95c6c3191d9150d2"
          }
        },
        "2f04fbadf5ea426b84019d75fcfdac1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d8bff5b844254695a135e35c4fa7e68b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102530333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102530333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc4ab7088f7c4d38860a1ad0d0142f0f"
          }
        },
        "a0d18009fa1f4778920b950fd7088082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_12984019af214d3cb661007b10fb74b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 161MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c3836f2b6bc4dd594484a81120efe86"
          }
        },
        "72e075fc048b4c53a4796d2fc46bb46d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25636c4c5e5840af95c6c3191d9150d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d8bff5b844254695a135e35c4fa7e68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc4ab7088f7c4d38860a1ad0d0142f0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12984019af214d3cb661007b10fb74b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c3836f2b6bc4dd594484a81120efe86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkushJamthikar/Image-Classification-Using-CNN/blob/main/DL_ImageClassification_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZLwyLcdbMqi",
        "outputId": "24fc24f8-dab8-4125-807e-abfbd4097fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-09 16:39:15--  https://storage.googleapis.com/kaggle-data-sets/7042/10119/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220209%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220209T124303Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=a746dc8f112c0a7fc537c09d722632959c88e5a9ae38810669a3d2a7be7f7ec85b5c97c2d15896c204d37cae8aa3187c3ba2a97fe634f92ed53758d8647d410a756f90330548d546697a471d934612bb43ade39e6cd7fd4cbc410e7ba1518f6548ccd94265bbc94a2d79ff855478146b6d43482944833f67d2d81c8115f7fa84e19ba416a15a60bbea6479d5a76e5124229527989d5c21e730382c4c65e97edf8ab600adb0b456735338b3b322ce2a160d8ee030d32095c6c01c7ac6da832535aa1751ba532b67d730ba3481ed0763283d5a94c3a0fbc3e8eddbcb6e4fc37059fc10d113ce53a6a6807c69f516843e762139d64f38e81cebcb2be9dfa65f5a4b\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.153.128, 142.250.145.128, 74.125.143.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.153.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94582348 (90M) [application/zip]\n",
            "Saving to: ‘archive.zip’\n",
            "\n",
            "archive.zip         100%[===================>]  90.20M  51.4MB/s    in 1.8s    \n",
            "\n",
            "2022-02-09 16:39:17 (51.4 MB/s) - ‘archive.zip’ saved [94582348/94582348]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-data-sets/7042/10119/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220209%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220209T124303Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=a746dc8f112c0a7fc537c09d722632959c88e5a9ae38810669a3d2a7be7f7ec85b5c97c2d15896c204d37cae8aa3187c3ba2a97fe634f92ed53758d8647d410a756f90330548d546697a471d934612bb43ade39e6cd7fd4cbc410e7ba1518f6548ccd94265bbc94a2d79ff855478146b6d43482944833f67d2d81c8115f7fa84e19ba416a15a60bbea6479d5a76e5124229527989d5c21e730382c4c65e97edf8ab600adb0b456735338b3b322ce2a160d8ee030d32095c6c01c7ac6da832535aa1751ba532b67d730ba3481ed0763283d5a94c3a0fbc3e8eddbcb6e4fc37059fc10d113ce53a6a6807c69f516843e762139d64f38e81cebcb2be9dfa65f5a4b\" -c -O 'archive.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For removing the folder\n",
        "#!rm -rf hymenoptera_data"
      ],
      "metadata": {
        "id": "EM7miW21jKZe"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!7z x 'archive.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETJVkhuQbaRZ",
        "outputId": "0575722c-8f22-4339-c792-ae8b2baaa4f2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.00GHz (50653),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 94582348 bytes (91 MiB)\n",
            "\n",
            "Extracting archive: archive.zip\n",
            "--\n",
            "Path = archive.zip\n",
            "Type = zip\n",
            "Physical Size = 94582348\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b 34% 293 - hymenoptera_data/hymenoptera_data/val/ants/477437164_bc3e6e594a.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 580 - hymenoptera_data/train/bees/2652877533_a564830cbf.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Files: 796\n",
            "Size:       94771848\n",
            "Compressed: 94582348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = '/content/hymenoptera_data/hymenoptera_data/val'\n",
        "dst = '/content/hymenoptera_data/hymenoptera_data/test'\n",
        "os.rename(src, dst)"
      ],
      "metadata": {
        "id": "q7Cc-34wjeUR"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import gc\n",
        "from torchvision                import models\n",
        "import torch.nn \t\t\t\t        as nn\n",
        "from torch.optim                import lr_scheduler\n",
        "import torch.optim \t\t\t\t      as optim"
      ],
      "metadata": {
        "id": "tCLFGoWJiDr4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "30T98uUqh2D4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_size     = 224\n",
        "pretrained_means    = [0.485, 0.456, 0.406] # \n",
        "pretrained_stds     = [0.229, 0.224, 0.225] # \n",
        "\n",
        "transform_train     = transforms.Compose([\n",
        "                                        transforms.RandomResizedCrop(pretrained_size),\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean = pretrained_means, \n",
        "                                                            std = pretrained_stds)\n",
        "                      ])\n",
        "\n",
        "transform_test      = transforms.Compose([\n",
        "                                        transforms.RandomResizedCrop(pretrained_size),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean = pretrained_means, \n",
        "                                                            std = pretrained_stds)]) "
      ],
      "metadata": {
        "id": "JYwFLyRViAWg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ImageFolder for Training and Testing Data\n",
        "TrainDataPath           = '/content/hymenoptera_data/train'\n",
        "TestDataPath            = '/content/hymenoptera_data/test'\n",
        "\n",
        "Traindataset            = datasets.ImageFolder(TrainDataPath, transform= transform_train)\n",
        "TestDataSet             = datasets.ImageFolder(TestDataPath, transform= transform_test)"
      ],
      "metadata": {
        "id": "lX9OVBMGhcVh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VALID_RATIO             = 0.9\n",
        "\n",
        "n_train_examples        = int(len(Traindataset) * VALID_RATIO)\n",
        "n_valid_examples        = len(Traindataset) - n_train_examples\n",
        "\n",
        "train_data, valid_data  = data.random_split(Traindataset, [n_train_examples, n_valid_examples])"
      ],
      "metadata": {
        "id": "Oc03IUTYilXH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Validdataset = copy.deepcopy(valid_data)\n",
        "Validdataset.dataset.transform = transform_test"
      ],
      "metadata": {
        "id": "ff9wLUNGi8n_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(TestDataSet)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3QhVeEljcEf",
        "outputId": "da768fca-34a6-459a-dade-3e0f2d67ae4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 219\n",
            "Number of validation examples: 25\n",
            "Number of testing examples: 153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Outputclasses     = 2\n",
        "batch_size        = 16\n",
        "EPOCHS            = 10\n",
        "best_valid_loss   = float('inf')\n",
        "cnn_net           = 'ResNet50'    \n",
        "\n"
      ],
      "metadata": {
        "id": "VbKjfiE2baOZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use the ImageFolder dataset to create the DataLoader\n",
        "Traindataloader = torch.utils.data.DataLoader(Traindataset, batch_size=batch_size, shuffle=True) \n",
        "Valdataloader   = torch.utils.data.DataLoader(Validdataset, batch_size=batch_size)\n",
        "    "
      ],
      "metadata": {
        "id": "C9pcbHgLhV0R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2inMK3xUkFof"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "K3rQtku3kLq9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ],
      "metadata": {
        "id": "QMinXTx-laqk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model    = models.resnet50(pretrained = True)\n",
        "net                 = pretrained_model\n",
        "print(f'The model has {count_parameters(pretrained_model):,} trainable parameters')\n",
        "for parameter in net.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "IN_FEATURES         = net.fc.in_features\n",
        "final_fc            = nn.Linear(IN_FEATURES, Outputclasses)\n",
        "net.fc              = final_fc    \n",
        "\n",
        "print(f'The model has {count_parameters(net):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "f9d054e7a7f943ad8aeac70c659cad27",
            "89463feb55b341bd9ef5b54f7622db33",
            "33bfc4d2244a46a08f03ab284ef40272",
            "2f04fbadf5ea426b84019d75fcfdac1c",
            "a0d18009fa1f4778920b950fd7088082",
            "72e075fc048b4c53a4796d2fc46bb46d",
            "25636c4c5e5840af95c6c3191d9150d2",
            "d8bff5b844254695a135e35c4fa7e68b",
            "cc4ab7088f7c4d38860a1ad0d0142f0f",
            "12984019af214d3cb661007b10fb74b5",
            "4c3836f2b6bc4dd594484a81120efe86"
          ]
        },
        "id": "Siaf_irvbaJw",
        "outputId": "b417865e-4667-4f82-a2c9-c375776b261f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9d054e7a7f943ad8aeac70c659cad27",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 25,557,032 trainable parameters\n",
            "The model has 4,098 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history     = [] \n",
        "START_LR    = 1e-7\n",
        "# optimizer   = optim.Adam(net.parameters(), lr = START_LR)\n",
        "optimizer   = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "criterion   = nn.CrossEntropyLoss()\n",
        "net         = net.to(device)\n",
        "criterion   = criterion.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW03aIqsbkQg",
        "outputId": "91c8fb1b-d463-49cf-a3ab-0e4015d3871b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, scheduler, device):\n",
        "    \n",
        "    epoch_loss       = 0\n",
        "    epoch_acc        = 0\n",
        "    epoch_precision  = 0\n",
        "    epoch_recall     = 0\n",
        "    epoch_f1         = 0\n",
        "    epoch_mcc        = 0\n",
        "    epoch_kappa      = 0\n",
        "    epoch_auc        = 0\n",
        "    # Set model in training mode\n",
        "    model.train()\n",
        "   \n",
        "    for (x, y) in iterator:\n",
        "        \n",
        "        # Send the x and y to device \n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        # clear all previous gradients \n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        # Make predictions using the model and input batch of image, x  \n",
        "        y_pred = model(x)\n",
        "        \n",
        "        # Compute loss between predictions and true labels \n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        # Compute accuracy\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "               \n",
        "        # # compute all performance metrics\n",
        "        # PE_df = function_performance_metrics(Initialize, y_pred, y)\n",
        "        \n",
        "        # Backpropogate loss and compute gradients\n",
        "        loss.backward()\n",
        "                \n",
        "        # Update parameters \n",
        "        optimizer.step()\n",
        "        \n",
        "        del x, y, y_pred\n",
        "        if (device == \"cuda:0\"):\n",
        "            torch.cuda.empty_cache()                    \n",
        "        else:\n",
        "            gc.collect()    # garbage collection\n",
        "        \n",
        "        # Compute the cumulative training loss and training accuracy\n",
        "        epoch_loss        += loss.item()\n",
        "        epoch_acc         += acc.item()\n",
        "        \n",
        "    scheduler.step()\n",
        "    \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "B8SkhveKlTzs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, iterator, criterion, device):\n",
        "    \n",
        "    epoch_loss       = 0\n",
        "    epoch_acc        = 0\n",
        "\n",
        "    # Set model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # do not backpropogate, compute gradient, and update weigths\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for (x, y) in iterator:\n",
        "\n",
        "            # Send the x and y to device \n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Make predictions using the model and input batch of image, x\n",
        "            y_pred = model(x)\n",
        "\n",
        "            # Compute loss between predictions and true labels \n",
        "            loss = criterion(y_pred, y)\n",
        "\n",
        "            # Compute accuracy\n",
        "            acc = calculate_accuracy(y_pred, y)\n",
        "                       \n",
        "            # Compute the cumulative training loss and training accuracy\n",
        "            epoch_loss        += loss.item()\n",
        "            epoch_acc         += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "x94nfxuslnM0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ModelDir    = 'model'\n",
        "Folderpath  = os.path.join('/content', ModelDir)\n",
        "isdir       = os.path.isdir(Folderpath)\n",
        "if(isdir==False):\n",
        "    os.mkdir(Folderpath) "
      ],
      "metadata": {
        "id": "DQpBPqf5mHxT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "start_trainingtime = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    start_time              = time.monotonic()\n",
        "    \n",
        "    train_loss, train_acc   = train(net, Traindataloader, optimizer, criterion, exp_lr_scheduler, device)\n",
        "    valid_loss, valid_acc   = validate(net, Valdataloader, criterion, device)\n",
        "    \n",
        "    # logg the training and validation history of loss and accuracy\n",
        "    history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss                     \n",
        "        SavePath  = os.path.join(Folderpath, 'checkpoint.pt')\n",
        "        torch.save(net.state_dict(), SavePath)\n",
        "\n",
        "    end_time = time.monotonic()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "end_trainingtime                = time.time()\n",
        "Training_mins, Training_secs    = epoch_time(start_trainingtime, end_trainingtime)\n",
        "print(f'Training and Validation Time: {Training_mins}m {Training_secs}s')       "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQQRrW2Fkq51",
        "outputId": "c7cb990c-ac82-446c-b256-8b37e77b508f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.735 | Train Acc: 47.27%\n",
            "\t Val. Loss: 0.512 |  Val. Acc: 61.46%\n",
            "Epoch: 02 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.494 | Train Acc: 70.70%\n",
            "\t Val. Loss: 0.311 |  Val. Acc: 90.62%\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.352 | Train Acc: 83.98%\n",
            "\t Val. Loss: 0.410 |  Val. Acc: 79.51%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.256 | Train Acc: 89.06%\n",
            "\t Val. Loss: 0.366 |  Val. Acc: 91.32%\n",
            "Epoch: 05 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.222 | Train Acc: 91.80%\n",
            "\t Val. Loss: 0.188 |  Val. Acc: 96.88%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.257 | Train Acc: 91.02%\n",
            "\t Val. Loss: 0.231 |  Val. Acc: 93.75%\n",
            "Epoch: 07 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.212 | Train Acc: 92.19%\n",
            "\t Val. Loss: 0.170 |  Val. Acc: 96.88%\n",
            "Epoch: 08 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.189 | Train Acc: 92.19%\n",
            "\t Val. Loss: 0.135 |  Val. Acc: 96.88%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.160 | Train Acc: 96.09%\n",
            "\t Val. Loss: 0.144 |  Val. Acc: 96.88%\n",
            "Epoch: 10 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.163 | Train Acc: 94.53%\n",
            "\t Val. Loss: 0.118 |  Val. Acc: 96.88%\n",
            "Training and Validation Time: 0m 30s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_paramters = torch.load('/content/model/checkpoint.pt')"
      ],
      "metadata": {
        "id": "KVfeSetklNCL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Newmodel    = models.resnet50(pretrained = False)\n",
        "print(f'The model has {count_parameters(pretrained_model):,} trainable parameters')\n",
        "for parameter in Newmodel.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "IN_FEATURES         = Newmodel.fc.in_features\n",
        "final_fc            = nn.Linear(IN_FEATURES, Outputclasses)\n",
        "Newmodel.fc         = final_fc "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azBVOifsYhHG",
        "outputId": "6636664f-deee-432a-fa4f-679d6fd332a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 4,098 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Newmodel.load_state_dict(trained_paramters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcPr6CSEXS0L",
        "outputId": "01b1042d-8bf7-48aa-fe45-bd8a32e4fc76"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Newmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc6b8WoLYIUc",
        "outputId": "b5a28510-2525-44b5-b455-3614a73a482a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Newmodel.layer4[2].bn3.weight.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy-JutEeYyM_",
        "outputId": "e57e51f4-e332-4d06-8149-c6adfcc206f7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2048])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FHXPeDx3ZAac"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}