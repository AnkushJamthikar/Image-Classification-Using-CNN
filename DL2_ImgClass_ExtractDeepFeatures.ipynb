{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkushJamthikar/Image-Classification-Using-CNN/blob/main/DL2_ImgClass_ExtractDeepFeatures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZLwyLcdbMqi",
        "outputId": "617b4ba4-ac06-4c71-8533-b9b1bdf0b765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-02-10 07:08:11--  https://storage.googleapis.com/kaggle-data-sets/7042/10119/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220209%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220209T124303Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=a746dc8f112c0a7fc537c09d722632959c88e5a9ae38810669a3d2a7be7f7ec85b5c97c2d15896c204d37cae8aa3187c3ba2a97fe634f92ed53758d8647d410a756f90330548d546697a471d934612bb43ade39e6cd7fd4cbc410e7ba1518f6548ccd94265bbc94a2d79ff855478146b6d43482944833f67d2d81c8115f7fa84e19ba416a15a60bbea6479d5a76e5124229527989d5c21e730382c4c65e97edf8ab600adb0b456735338b3b322ce2a160d8ee030d32095c6c01c7ac6da832535aa1751ba532b67d730ba3481ed0763283d5a94c3a0fbc3e8eddbcb6e4fc37059fc10d113ce53a6a6807c69f516843e762139d64f38e81cebcb2be9dfa65f5a4b\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.18.128, 142.250.153.128, 142.250.145.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.18.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94582348 (90M) [application/zip]\n",
            "Saving to: ‘archive.zip’\n",
            "\n",
            "archive.zip         100%[===================>]  90.20M  56.8MB/s    in 1.6s    \n",
            "\n",
            "2022-02-10 07:08:13 (56.8 MB/s) - ‘archive.zip’ saved [94582348/94582348]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-data-sets/7042/10119/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220209%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220209T124303Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=a746dc8f112c0a7fc537c09d722632959c88e5a9ae38810669a3d2a7be7f7ec85b5c97c2d15896c204d37cae8aa3187c3ba2a97fe634f92ed53758d8647d410a756f90330548d546697a471d934612bb43ade39e6cd7fd4cbc410e7ba1518f6548ccd94265bbc94a2d79ff855478146b6d43482944833f67d2d81c8115f7fa84e19ba416a15a60bbea6479d5a76e5124229527989d5c21e730382c4c65e97edf8ab600adb0b456735338b3b322ce2a160d8ee030d32095c6c01c7ac6da832535aa1751ba532b67d730ba3481ed0763283d5a94c3a0fbc3e8eddbcb6e4fc37059fc10d113ce53a6a6807c69f516843e762139d64f38e81cebcb2be9dfa65f5a4b\" -c -O 'archive.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM7miW21jKZe"
      },
      "outputs": [],
      "source": [
        "# For removing the folder\n",
        "#!rm -rf hymenoptera_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETJVkhuQbaRZ",
        "outputId": "cdc2a976-3aa3-4a38-c0cc-6796a6bbcc52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.00GHz (50653),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 94582348 bytes (91 MiB)\n",
            "\n",
            "Extracting archive: archive.zip\n",
            "--\n",
            "Path = archive.zip\n",
            "Type = zip\n",
            "Physical Size = 94582348\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b 36% 304 - hymenoptera_data/hymenoptera_d . r_ant_qeen_excavating_hole.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 596 - hymenoptera_data/train/bees/2908916142_a7ac8b57a8.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Files: 796\n",
            "Size:       94771848\n",
            "Compressed: 94582348\n"
          ]
        }
      ],
      "source": [
        "!7z x 'archive.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCLFGoWJiDr4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import gc\n",
        "from torchvision                import models\n",
        "import torch.nn \t\t\t\t        as nn\n",
        "from torch.optim                import lr_scheduler\n",
        "import torch.optim \t\t\t\t      as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7Cc-34wjeUR"
      },
      "outputs": [],
      "source": [
        "src = '/content/hymenoptera_data/hymenoptera_data/val'\n",
        "dst = '/content/hymenoptera_data/hymenoptera_data/test'\n",
        "os.rename(src, dst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30T98uUqh2D4"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYwFLyRViAWg"
      },
      "outputs": [],
      "source": [
        "pretrained_size     = 224\n",
        "pretrained_means    = [0.485, 0.456, 0.406] # \n",
        "pretrained_stds     = [0.229, 0.224, 0.225] # \n",
        "\n",
        "transform_train     = transforms.Compose([\n",
        "                                        transforms.RandomResizedCrop(pretrained_size),\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean = pretrained_means, \n",
        "                                                            std = pretrained_stds)\n",
        "                      ])\n",
        "\n",
        "transform_test      = transforms.Compose([\n",
        "                                        transforms.RandomResizedCrop(pretrained_size),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean = pretrained_means, \n",
        "                                                            std = pretrained_stds)]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX9OVBMGhcVh"
      },
      "outputs": [],
      "source": [
        "# Create ImageFolder for Training and Testing Data\n",
        "TrainDataPath           = '/content/hymenoptera_data/hymenoptera_data/train'\n",
        "TestDataPath            = '/content/hymenoptera_data/hymenoptera_data/test'\n",
        "\n",
        "Traindataset            = datasets.ImageFolder(TrainDataPath, transform= transform_train)\n",
        "TestDataSet             = datasets.ImageFolder(TestDataPath, transform= transform_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc03IUTYilXH"
      },
      "outputs": [],
      "source": [
        "VALID_RATIO             = 0.9\n",
        "\n",
        "n_train_examples        = int(len(Traindataset) * VALID_RATIO)\n",
        "n_valid_examples        = len(Traindataset) - n_train_examples\n",
        "\n",
        "train_data, valid_data  = data.random_split(Traindataset, [n_train_examples, n_valid_examples])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff9wLUNGi8n_"
      },
      "outputs": [],
      "source": [
        "Validdataset = copy.deepcopy(valid_data)\n",
        "Validdataset.dataset.transform = transform_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3QhVeEljcEf",
        "outputId": "e0d34000-30ff-4582-c3f0-dd0a2bf1986f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 219\n",
            "Number of validation examples: 25\n",
            "Number of testing examples: 153\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(TestDataSet)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbKjfiE2baOZ"
      },
      "outputs": [],
      "source": [
        "Outputclasses     = 2\n",
        "batch_size        = 16\n",
        "EPOCHS            = 10\n",
        "best_valid_loss   = float('inf')\n",
        "cnn_net           = 'ResNet50'    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9pcbHgLhV0R"
      },
      "outputs": [],
      "source": [
        "# TODO: use the ImageFolder dataset to create the DataLoader\n",
        "Traindataloader = torch.utils.data.DataLoader(Traindataset, batch_size=batch_size, shuffle=True) \n",
        "Valdataloader   = torch.utils.data.DataLoader(Validdataset, batch_size=batch_size)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3rQtku3kLq9"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMinXTx-laqk"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "eb387a7b388f4f6e9aa648a90c7d6864",
            "6cc9527560e0464a90762c575314579c",
            "013088347ae74c19abf4b6ab539c8a6a",
            "22fc9645b1844f79ad05cf6edae01890",
            "8c19e3843dd048f0a491dfca72a50ad1",
            "77f9b0f488af40aaaf4a087164399489",
            "a9a9970ccba84a5bb1e64702e163ec49",
            "0f4b9b97e5d04f4a967f1abaee009968",
            "ec8fca34daf04c4488d5073424e28502",
            "ba9a0ba07ca74a70bc264c938af06695",
            "6e197694b225472b833a50755e554393"
          ]
        },
        "id": "Siaf_irvbaJw",
        "outputId": "ae163ee8-8e15-41fe-c352-98f13b2ab916"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb387a7b388f4f6e9aa648a90c7d6864",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 25,557,032 trainable parameters\n",
            "The model has 4,098 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "pretrained_model    = models.resnet50(pretrained = True)\n",
        "net                 = pretrained_model\n",
        "print(f'The model has {count_parameters(pretrained_model):,} trainable parameters')\n",
        "for parameter in net.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "IN_FEATURES         = net.fc.in_features\n",
        "final_fc            = nn.Linear(IN_FEATURES, Outputclasses)\n",
        "net.fc              = final_fc    \n",
        "\n",
        "print(f'The model has {count_parameters(net):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW03aIqsbkQg",
        "outputId": "6d79588a-0647-44c0-b5d7-551f8506f706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "history     = [] \n",
        "START_LR    = 1e-7\n",
        "# optimizer   = optim.Adam(net.parameters(), lr = START_LR)\n",
        "optimizer   = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "criterion   = nn.CrossEntropyLoss()\n",
        "net         = net.to(device)\n",
        "criterion   = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8SkhveKlTzs"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, scheduler, device):\n",
        "    \n",
        "    epoch_loss       = 0\n",
        "    epoch_acc        = 0\n",
        "    epoch_precision  = 0\n",
        "    epoch_recall     = 0\n",
        "    epoch_f1         = 0\n",
        "    epoch_mcc        = 0\n",
        "    epoch_kappa      = 0\n",
        "    epoch_auc        = 0\n",
        "    # Set model in training mode\n",
        "    model.train()\n",
        "   \n",
        "    for (x, y) in iterator:\n",
        "        \n",
        "        # Send the x and y to device \n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        # clear all previous gradients \n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        # Make predictions using the model and input batch of image, x  \n",
        "        y_pred = model(x)\n",
        "        \n",
        "        # Compute loss between predictions and true labels \n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        # Compute accuracy\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "               \n",
        "        # # compute all performance metrics\n",
        "        # PE_df = function_performance_metrics(Initialize, y_pred, y)\n",
        "        \n",
        "        # Backpropogate loss and compute gradients\n",
        "        loss.backward()\n",
        "                \n",
        "        # Update parameters \n",
        "        optimizer.step()\n",
        "        \n",
        "        del x, y, y_pred\n",
        "        if (device == \"cuda:0\"):\n",
        "            torch.cuda.empty_cache()                    \n",
        "        else:\n",
        "            gc.collect()    # garbage collection\n",
        "        \n",
        "        # Compute the cumulative training loss and training accuracy\n",
        "        epoch_loss        += loss.item()\n",
        "        epoch_acc         += acc.item()\n",
        "        \n",
        "    scheduler.step()\n",
        "    \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x94nfxuslnM0"
      },
      "outputs": [],
      "source": [
        "def validate(model, iterator, criterion, device):\n",
        "    \n",
        "    epoch_loss       = 0\n",
        "    epoch_acc        = 0\n",
        "\n",
        "    # Set model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # do not backpropogate, compute gradient, and update weigths\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for (x, y) in iterator:\n",
        "\n",
        "            # Send the x and y to device \n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Make predictions using the model and input batch of image, x\n",
        "            y_pred = model(x)\n",
        "\n",
        "            # Compute loss between predictions and true labels \n",
        "            loss = criterion(y_pred, y)\n",
        "\n",
        "            # Compute accuracy\n",
        "            acc = calculate_accuracy(y_pred, y)\n",
        "                       \n",
        "            # Compute the cumulative training loss and training accuracy\n",
        "            epoch_loss        += loss.item()\n",
        "            epoch_acc         += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQpBPqf5mHxT"
      },
      "outputs": [],
      "source": [
        "ModelDir    = 'model'\n",
        "Folderpath  = os.path.join('/content', ModelDir)\n",
        "isdir       = os.path.isdir(Folderpath)\n",
        "if(isdir==False):\n",
        "    os.mkdir(Folderpath) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQQRrW2Fkq51",
        "outputId": "4790c829-46aa-49dc-a2ad-0142605023df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.735 | Train Acc: 47.27%\n",
            "\t Val. Loss: 0.512 |  Val. Acc: 61.46%\n",
            "Epoch: 02 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.494 | Train Acc: 70.70%\n",
            "\t Val. Loss: 0.311 |  Val. Acc: 90.62%\n",
            "Epoch: 03 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.352 | Train Acc: 83.98%\n",
            "\t Val. Loss: 0.410 |  Val. Acc: 79.51%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.256 | Train Acc: 89.06%\n",
            "\t Val. Loss: 0.366 |  Val. Acc: 91.32%\n",
            "Epoch: 05 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.222 | Train Acc: 91.80%\n",
            "\t Val. Loss: 0.188 |  Val. Acc: 96.88%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.257 | Train Acc: 91.02%\n",
            "\t Val. Loss: 0.231 |  Val. Acc: 93.75%\n",
            "Epoch: 07 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.212 | Train Acc: 92.19%\n",
            "\t Val. Loss: 0.170 |  Val. Acc: 96.88%\n",
            "Epoch: 08 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.189 | Train Acc: 92.19%\n",
            "\t Val. Loss: 0.135 |  Val. Acc: 96.88%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.160 | Train Acc: 96.09%\n",
            "\t Val. Loss: 0.144 |  Val. Acc: 96.88%\n",
            "Epoch: 10 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.163 | Train Acc: 94.53%\n",
            "\t Val. Loss: 0.118 |  Val. Acc: 96.88%\n",
            "Training and Validation Time: 0m 30s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "start_trainingtime = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    start_time              = time.monotonic()\n",
        "    \n",
        "    train_loss, train_acc   = train(net, Traindataloader, optimizer, criterion, exp_lr_scheduler, device)\n",
        "    valid_loss, valid_acc   = validate(net, Valdataloader, criterion, device)\n",
        "    \n",
        "    # logg the training and validation history of loss and accuracy\n",
        "    history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss                     \n",
        "        SavePath  = os.path.join(Folderpath, 'checkpoint.pt')\n",
        "        torch.save(net.state_dict(), SavePath)\n",
        "\n",
        "    end_time = time.monotonic()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "end_trainingtime                = time.time()\n",
        "Training_mins, Training_secs    = epoch_time(start_trainingtime, end_trainingtime)\n",
        "print(f'Training and Validation Time: {Training_mins}m {Training_secs}s')       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVfeSetklNCL"
      },
      "outputs": [],
      "source": [
        "trained_paramters = torch.load('/content/model/checkpoint.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azBVOifsYhHG",
        "outputId": "953ad0d1-acdd-4616-f9fa-23d50c78f03a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 4,098 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "ResNet_SavedModel    = models.resnet50(pretrained = False)\n",
        "print(f'The model has {count_parameters(pretrained_model):,} trainable parameters')\n",
        "for parameter in ResNet_SavedModel.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "IN_FEATURES         = ResNet_SavedModel.fc.in_features\n",
        "final_fc            = nn.Linear(IN_FEATURES, Outputclasses)\n",
        "ResNet_SavedModel.fc         = final_fc "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcPr6CSEXS0L",
        "outputId": "a5526dfd-a7a6-4548-911e-a4415ca91426"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ResNet_SavedModel.load_state_dict(trained_paramters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IElUkmBGexas"
      },
      "source": [
        "# Extracting the Deep Features from Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHXPeDx3ZAac"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "\t\t# Extract Feature Layers\n",
        "    self.features = list(model.children())[:-1]      # delete the last fc layer.\n",
        "    self.features = nn.Sequential(*self.features)\n",
        "\t\t# Extract Average Pooling Layer\n",
        "    self.pooling = model.avgpool\n",
        "\t\t# Convert the image into one-dimensional vector\n",
        "    self.flatten = nn.Flatten()\n",
        "\t\t# Extract the first part of fully-connected layer \n",
        "    #self.fc = model.classifier[0]\n",
        "  \n",
        "  def forward(self, x):\n",
        "\t\t# It will take the input 'x' until it returns the feature vector called 'out'\n",
        "    out = self.features(x)\n",
        "    out = self.pooling(out)\n",
        "    out = self.flatten(out)\n",
        "    # out = self.fc(out) # We do not need, because we are taking all features after flattening\n",
        "    return out "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LLq8N7nwEu_"
      },
      "outputs": [],
      "source": [
        "new_model = FeatureExtractor(ResNet_SavedModel)\n",
        "# Change the device to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
        "new_model = new_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "webBGWuYwKm-"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import glob\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf9hIqMY5Bdl",
        "outputId": "298623d8-ef2b-4077-cbe8-c4936fe6c19c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "listOfimgs    = glob.glob(\"/content/hymenoptera_data/hymenoptera_data/test/ants/*.jpg\")\n",
        "N             = len(listOfimgs)\n",
        "N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiLkGPJFxnjd",
        "outputId": "97ead476-3212-4ff3-cdaa-24598d53519f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70/70 [00:01<00:00, 47.94it/s]\n"
          ]
        }
      ],
      "source": [
        "# Transform the image, so it becomes readable with the model\n",
        "transform = transforms.Compose([\n",
        "  transforms.ToPILImage(),\n",
        "  transforms.RandomResizedCrop(pretrained_size),\n",
        "  #transforms.CenterCrop(512),\n",
        "  #transforms.Resize(448),\n",
        "  transforms.ToTensor(),            \n",
        "  transforms.Normalize(mean = pretrained_means, std = pretrained_stds)])                   \n",
        "\n",
        "# Will contain the feature\n",
        "features = []\n",
        "# Iterate each image\n",
        "for i in tqdm(range(N)):\n",
        "  path    = listOfimgs[i]\n",
        "  img     = cv2.imread(path)\n",
        "  img     = transform(img)\n",
        "  img     = torch.unsqueeze(img, 0)\n",
        "  \n",
        "  img = img.to(device)\n",
        "  # We only extract features, so we don't need gradient\n",
        "  with torch.no_grad():\n",
        "\t\t# Extract the feature from the image\n",
        "    feature = new_model(img)\n",
        "  feature = torch.squeeze(feature, 0)\n",
        "  features.append(feature.cpu().detach().numpy())\n",
        "\n",
        "# Convert to NumPy Array\n",
        "DeepFeatures = np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW2wyFr16LGT",
        "outputId": "6612ce77-9f8d-4b9c-ea45-2c057599c840"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.37116888, 0.5400069 , 0.4440094 , ..., 0.39446625, 0.4284522 ,\n",
              "        0.30777255],\n",
              "       [0.3091018 , 0.55764663, 0.513645  , ..., 0.3506437 , 0.43005916,\n",
              "        0.3439475 ],\n",
              "       [0.3128039 , 0.48155913, 0.43391994, ..., 0.37289765, 0.44517568,\n",
              "        0.32935947],\n",
              "       ...,\n",
              "       [0.40981916, 0.56374806, 0.48027402, ..., 0.30278757, 0.45008639,\n",
              "        0.29286525],\n",
              "       [0.3544068 , 0.56110394, 0.5004604 , ..., 0.32892615, 0.42215425,\n",
              "        0.3415575 ],\n",
              "       [0.3839808 , 0.4686308 , 0.4961139 , ..., 0.36172712, 0.4332603 ,\n",
              "        0.33661214]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "DeepFeatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH9L4sV06LQ6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#References\n",
        "1) https://towardsdatascience.com/image-feature-extraction-using-pytorch-e3b327c3607a"
      ],
      "metadata": {
        "id": "mrGGL8wu--kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p9iFVGjk_AC9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL2-ImgClass-ExtractDeepFeatures.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNnPdqNdaMqA1tar247BF6y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "013088347ae74c19abf4b6ab539c8a6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f4b9b97e5d04f4a967f1abaee009968",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec8fca34daf04c4488d5073424e28502",
            "value": 102530333
          }
        },
        "0f4b9b97e5d04f4a967f1abaee009968": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22fc9645b1844f79ad05cf6edae01890": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba9a0ba07ca74a70bc264c938af06695",
            "placeholder": "​",
            "style": "IPY_MODEL_6e197694b225472b833a50755e554393",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 195MB/s]"
          }
        },
        "6cc9527560e0464a90762c575314579c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77f9b0f488af40aaaf4a087164399489",
            "placeholder": "​",
            "style": "IPY_MODEL_a9a9970ccba84a5bb1e64702e163ec49",
            "value": "100%"
          }
        },
        "6e197694b225472b833a50755e554393": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77f9b0f488af40aaaf4a087164399489": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c19e3843dd048f0a491dfca72a50ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9a9970ccba84a5bb1e64702e163ec49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba9a0ba07ca74a70bc264c938af06695": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb387a7b388f4f6e9aa648a90c7d6864": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cc9527560e0464a90762c575314579c",
              "IPY_MODEL_013088347ae74c19abf4b6ab539c8a6a",
              "IPY_MODEL_22fc9645b1844f79ad05cf6edae01890"
            ],
            "layout": "IPY_MODEL_8c19e3843dd048f0a491dfca72a50ad1"
          }
        },
        "ec8fca34daf04c4488d5073424e28502": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}